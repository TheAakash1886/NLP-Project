# -*- coding: utf-8 -*-
"""CS513_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bLzsHyT3yoo96M7w-qxTEmwxSTtueT-5
"""

from google.colab import drive
drive.mount('/content/drive')



#Standard Python Library

import pandas as pd
import numpy as np
import seaborn as sns
import json
import os

#Standar NLP Pre-Processing Library

import nltk as nlp
from nltk.corpus import stopwords
import string
import re
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn import feature_extraction, linear_model, model_selection, preprocessing
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB,BernoulliNB,MultinomialNB,MultinomialNB


#SK Learn and TF library for NLP NueralNet Layers
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer
from sklearn.gaussian_process import GaussianProcessClassifier
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score

from tensorflow.keras.utils import plot_model 

#Logistic Regression Library
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier

#Naive Bayes Library
from sklearn.naive_bayes import GaussianNB

#KNN Libraries
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix


#LSTM Libraries
from tensorflow.keras.layers import BatchNormalization, LSTM, GRU
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import optimizers
import tensorflow.keras.backend as backend
from nltk import word_tokenize, KneserNeyProbDist, SimpleGoodTuringProbDist, FreqDist, trigrams

"""Setting up Juypter Notebook template for Project

"""

"""
READS Text File and return all of body as a string
"""
def read_input(input_path:str) -> str:
    file_data = open(input_path , 'r')
    
    return file_data.read()

"""
READS Directory Text File and return list of each text file
"""
def read_directory(input_dir:str):
  data = []
  files = [f for f in os.listdir(input_dir)] #if os.path.isfile(f)]
  for f in files:
    f_path = os.path.join(input_dir, f)
    with open (f_path, "r") as myfile:
      data.append(myfile.read())
  return data

#The Four Group of Data
train_pos = read_directory('/content/drive/MyDrive/aclImdb/train/pos')
train_neg = read_directory('/content/drive/MyDrive/aclImdb/train/neg')
test_pos = read_directory('/content/drive/MyDrive/aclImdb/test/pos')
test_neg = read_directory('/content/drive/MyDrive/aclImdb/test/pos')

"""
Text pre-processing function
"""
def preprocess(paragraph, label, sample_size):
 
    data_set = [paragraph.strip() for paragraph in paragraphs if len(paragraph) > sample_size]
    data = [re.sub('[\W_]+', ' ', sample.lower().strip()) for sample in data_set]
    size = len(data)

    label_array = np.ones((size,)) * label
    df = pd.DataFrame({'paragraph': data, 'category':label_array })
    print('The total number of examples for category ' + str(label)+ ' is: ' + str(size))
    return df, size

X = pd.concat([pd.DataFrame(np.array(train_pos)),pd.DataFrame(np.array(train_neg))])
X.shape
X2 = pd.concat([pd.DataFrame(np.array(test_pos)),pd.DataFrame(np.array(test_neg))])
X2.shape

df = pd.DataFrame(np.ones((len(train_pos),1)))
df2 = pd.DataFrame(np.zeros((len(train_neg),1)))
Y = pd.concat([df,df2])
Y.shape

df = pd.DataFrame(np.ones((len(test_pos),1)))
df2 = pd.DataFrame(np.zeros((len(test_neg),1)))
Y2 = pd.concat([df,df2])
Y2.shape

vocab = read_input('/content/drive/MyDrive/aclImdb/imdb.vocab').split('\n')
vocab_size = len(vocab) + 1

tokenizer = Tokenizer(num_words=3000)
tokenizer.fit_on_texts(X[0])

X_train = tokenizer.texts_to_sequences(X[0])
X_test = tokenizer.texts_to_sequences(X2[0])

maxlen = 100

X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)
X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)

label_train = to_categorical(Y)
label_test = to_categorical(Y2)

vocab = read_input('/content/drive/MyDrive/aclImdb/imdb.vocab').split('\n')
vocab_size = len(vocab) + 1

# LSTM
inputs = Input(shape=(maxlen,))
lstm_model = Sequential()
lstm_model.add(Embedding(vocab_size, 50))
lstm_model.add(LSTM(10))
lstm_model.add(Dense(2, activation='relu'))
print(lstm_model.summary())

def perplexity(predict, label):
    return backend.exp(backend.categorical_crossentropy(predict, label))

opt = optimizers.Adam(learning_rate=0.001)

lstm_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[perplexity])

lstm_model.fit(X_train, label_train, epochs=100, batch_size=64)

lstm_loss = lstm_model.evaluate(X_test, label_test)
loss = lstm_loss[0]
perplexity_test = lstm_loss[1]
print('loss = ' + str(loss))
print('perplexity = ' + str(perplexity_test))

# Logistic Regression
lr_model = LogisticRegression()
lr_model.fit(X_train, y_train)
lr_pred = lr_model.predict(X_test)
lr_acc = accuracy_score(y_test, lr_pred)
lr_cm = confusion_matrix(y_test, lr_pred)
print("Logistic Regression Accuracy: " + str(lr_acc))
print("Logistic Regression Confusion Matrix: ", lr_cm)

# Aniket: I have made some changes.
# Please comment if any changes are required.

from google.colab import drive
drive.mount('/content/drive')
!unzip drive/MyDrive/aclImdb.zip

import os
import random
import numpy as np
import pandas as pd
import nltk

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

from nltk.tokenize import sent_tokenize, word_tokenize 
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer 
from nltk.corpus import wordnet
from nltk.tag import pos_tag
import re

from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix
import itertools
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

!pip3 install sentence-transformers
from sentence_transformers import SentenceTransformer
import gensim 
from gensim.models import Word2Vec

"""
Text pre-processing function
"""
def preprocess(doc, label, sample_size):
    data_list=[]
    for paragraphs in doc.values:
      #print(paragraphs[0])
      da = paragraphs[0].split('\n\n')
      data_set = [paragraph.strip() for paragraph in da if len(paragraph) > sample_size]
      data = [re.sub('[\W_]+', ' ', sample.lower().strip()) for sample in data_set]
      if len(data)>0:
        data_list.append(data[0])
    size = len(data_list)

    label_array = np.ones((size,)) * label
    df = pd.DataFrame({'paragraph': data_list, 'category':label_array })
    print('The total number of examples for category ' + str(label)+ ' is: ' + str(size))
    return df, size

"""
READS Text File and return all of body as a string
"""


def read_input(input_path:str) -> str:
    file_data = open(input_path , 'r')
    
    return file_data.read()

def read_directory(input_dir:str):
  data = []
  #print(input_dir)
  files = [f for f in os.listdir(input_dir)]
  print(files)
  for x in range(1000):
  #for f in files:
    #print("entered")
    #with open (input_dir+"/"+f, "r") as myfile:
    with open (input_dir+"/"+files[x], "r") as myfile:
      # print(myfile.read())
      data.append(myfile.read())

  df = pd.DataFrame(data)
  return df

train_pos = read_directory('/content/aclImdb/train/pos')
train_neg = read_directory('/content/aclImdb/train/neg')
test_pos = read_directory('/content/aclImdb/test/pos')
test_neg = read_directory('/content/aclImdb/test/neg')
#read_directory('/content/aclImdb/train/pos')

def mean(z): # used for BERT (word version) and Word2Vec
    return sum(itertools.chain(z))/len(z)

def cleanText(text):
    
    text = re.sub(r'<.*?>', ' ', text)
    text = re.sub(r"won't", "will not", text)
    text = re.sub(r"can't", "can not", text)
    text = re.sub(r"n't", " not", text)
    text = re.sub(r"'ve", " have", text)
    text = re.sub(r"'ll", " will", text)
    text = re.sub(r"'re", " are", text)

    
    # Replace punctuations with space
    # save ! ? . for end of the sentence detection [,/():;']
    filters='"#$%&*+<=>@[\\]^_`{|}~\t\n'
    text = re.sub(r'\!+', '!', text)
    text = re.sub(r'\?+', '?', text)

    translate_dict = dict((i, " ") for i in filters)
    translate_map = str.maketrans(translate_dict)
    text = text.translate(translate_map)
    
    
    text = re.sub(r'\( *\)', ' ', text)

    # Replace multiple space with one space
    text = re.sub(' +', ' ', text)
    
    text = ''.join(text)

    return text

def embeddToBERT(text):
    sentences = re.split('!|\?|\.',text)
    sentences = list(filter(None, sentences)) 
    
    ## encoding the sentence
    result = bert_transformers.encode(sentences)
    #sys.stdout.write('\r'+"in")
    feature = [mean(x) for x in zip(*result)]
  
    return np.asarray(feature).reshape((768,1))

embedding = 'BERT'
# for Word2Vec with stop words
train_pos['clean_text'] = train_pos[0].apply(cleanText)
train_neg['clean_text'] = train_neg[0].apply(cleanText)
test_pos['clean_text'] = test_pos[0].apply(cleanText)
test_neg['clean_text'] = test_neg[0].apply(cleanText)

train_pos["label"]=1
train_neg["label"]=0
test_pos["label"]=1
test_neg["label"]=0

frames = [train_pos, train_neg]
train_data = pd.concat(frames)
frames = [test_pos, test_neg]
test_data = pd.concat(frames)

X_train, X_test, y_train, y_test = train_test_split(train_data['clean_text'], train_data['label'], test_size=0.33, random_state=321)
X_train.head()

ert_transformers = SentenceTransformer('bert-base-nli-mean-tokens')

import sys
#bert_versio= 'SENTENCE'
c=0
bert_sentence_training_features=[]
for i in X_train:
    sys.stdout.write('\r'+str(c))
    bert_sentence_training_features.append(embeddToBERT(i))
    c=c+1

feature = [x.T for x in bert_sentence_training_features]
bert_sentence_training_features = np.asarray(feature).reshape(len(X_train),768)

print(bert_sentence_training_features.shape)

# bert_sentence_test_features = X_test.apply(embeddToBERT)
#bert_version = 'SENTENCE'
c=0
bert_sentence_test_features=[]
for i in X_test:
    sys.stdout.write('\r'+str(c))
    bert_sentence_test_features.append(embeddToBERT(i))
    c=c+1

feature = [x.T for x in bert_sentence_test_features]
bert_sentence_test_features = np.asarray(feature).reshape(len(X_test),768)
print(bert_sentence_test_features.shape)

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report

clf = GaussianNB()
clf.fit(bert_sentence_training_features, y_train)
y_pred=clf.predict(bert_sentence_test_features)
print(classification_report(y_test, y_pred))

# using for tfidf

from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.decomposition import TruncatedSVD

paragraphs = X.values
categorys = y

sentences_train, sentences_test, y_train, y_test = train_test_split(paragraphs, categorys, test_size=0.20, random_state=11)
tf_idf_vect = TfidfVectorizer()
tf_idf_train = tf_idf_vect.fit_transform(sentences_train)
tf_idf_test = tf_idf_vect.transform(sentences_test)

truncatedSVD=TruncatedSVD(5000)
X_truncated = truncatedSVD.fit_transform(tf_idf_train)

X_truncated_test=truncatedSVD.transform(tf_idf_test)

clf = GaussianNB()
clf.fit(X_truncated, y_train)
y_pred=clf.predict(X_truncated_test)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

#KNN Classifier
knn = KNeighborsClassifier(n_neighbors=3)
clf = knn.fit(X_train, y_train)
predicted = clf.predict(X_test)
print(predicted)
print('Confusion Matrix: ',confusion_matrix(y_test,predicted), sep = '')
print('Accuracy Score: ',accuracy_score(y_test,predicted)*100,'%',sep='\n')





